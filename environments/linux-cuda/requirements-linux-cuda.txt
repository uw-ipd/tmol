#
# This file is autogenerated by pip-compile with Python 3.11
# by the following command:
#
#    pip-compile --no-emit-index-url --output-file=requirements-linux-cuda.txt ../../requirements.in
#
--trusted-host data.pyg.org

asciitree==0.3.3
    # via zarr
astor==0.8.1
    # via -r ../../requirements.in
asttokens==2.4.1
    # via stack-data
attrs==23.2.0
    # via
    #   -r ../../requirements.in
    #   attrs-strict
    #   cattrs
    #   hypothesis
attrs-strict==1.0.1
    # via -r ../../requirements.in
cattrs==23.2.3
    # via -r ../../requirements.in
certifi==2024.2.2
    # via requests
charset-normalizer==3.3.2
    # via requests
decorator==5.1.1
    # via
    #   -r ../../requirements.in
    #   ipython
executing==2.0.1
    # via stack-data
fasteners==0.19
    # via zarr
filelock==3.13.1
    # via
    #   torch
    #   triton
frozendict==2.4.0
    # via -r ../../requirements.in
fsspec==2024.2.0
    # via torch
hypothesis==6.98.3
    # via -r ../../requirements.in
idna==3.6
    # via requests
ipython==8.21.0
    # via -r ../../requirements.in
jedi==0.19.1
    # via ipython
jinja2==3.1.3
    # via torch
llvmlite==0.42.0
    # via numba
markupsafe==2.1.5
    # via jinja2
matplotlib-inline==0.1.6
    # via ipython
mpmath==1.3.0
    # via sympy
mypy-extensions==1.0.0
    # via typing-inspect
networkx==3.2.1
    # via
    #   -r ../../requirements.in
    #   torch
ninja==1.11.1.1
    # via -r ../../requirements.in
numba==0.59.0
    # via sparse
numcodecs==0.12.1
    # via zarr
numpy==1.26.4
    # via
    #   -r ../../requirements.in
    #   numba
    #   numcodecs
    #   pandas
    #   pyarrow
    #   scipy
    #   sparse
    #   zarr
nvidia-cublas-cu12==12.1.3.1
    # via
    #   nvidia-cudnn-cu12
    #   nvidia-cusolver-cu12
    #   torch
nvidia-cuda-cupti-cu12==12.1.105
    # via torch
nvidia-cuda-nvrtc-cu12==12.1.105
    # via torch
nvidia-cuda-runtime-cu12==12.1.105
    # via torch
nvidia-cudnn-cu12==8.9.2.26
    # via torch
nvidia-cufft-cu12==11.0.2.54
    # via torch
nvidia-curand-cu12==10.3.2.106
    # via torch
nvidia-cusolver-cu12==11.4.5.107
    # via torch
nvidia-cusparse-cu12==12.1.0.106
    # via
    #   nvidia-cusolver-cu12
    #   torch
nvidia-nccl-cu12==2.19.3
    # via torch
nvidia-nvjitlink-cu12==12.3.101
    # via
    #   nvidia-cusolver-cu12
    #   nvidia-cusparse-cu12
nvidia-nvtx-cu12==12.1.105
    # via torch
pandas==2.2.0
    # via -r ../../requirements.in
parso==0.8.3
    # via jedi
pexpect==4.9.0
    # via ipython
pint==0.23
    # via -r ../../requirements.in
prompt-toolkit==3.0.43
    # via ipython
psutil==5.9.8
    # via -r ../../requirements.in
ptyprocess==0.7.0
    # via pexpect
pure-eval==0.2.2
    # via stack-data
pyarrow==15.0.0
    # via -r ../../requirements.in
pygments==2.17.2
    # via ipython
python-dateutil==2.8.2
    # via pandas
pytz==2024.1
    # via pandas
pyyaml==6.0.1
    # via -r ../../requirements.in
requests==2.31.0
    # via -r ../../requirements.in
scipy==1.12.0
    # via
    #   -r ../../requirements.in
    #   sparse
six==1.16.0
    # via
    #   asttokens
    #   python-dateutil
sortedcontainers==2.4.0
    # via hypothesis
sparse==0.15.1
    # via -r ../../requirements.in
stack-data==0.6.3
    # via ipython
sympy==1.12
    # via torch
toolz==0.12.1
    # via -r ../../requirements.in
torch==2.2.0
    # via -r ../../requirements.in
traitlets==5.14.1
    # via
    #   ipython
    #   matplotlib-inline
triton==2.2.0
    # via torch
typing-extensions==4.9.0
    # via
    #   -r ../../requirements.in
    #   pint
    #   torch
    #   typing-inspect
typing-inspect==0.9.0
    # via -r ../../requirements.in
tzdata==2023.4
    # via pandas
urllib3==2.2.0
    # via requests
wcwidth==0.2.13
    # via prompt-toolkit
zarr==2.16.1
    # via -r ../../requirements.in
